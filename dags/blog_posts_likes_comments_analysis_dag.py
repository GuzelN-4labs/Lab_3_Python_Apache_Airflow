# -*- coding: utf-8 -*-
"""blog_posts_db.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C_q9DwAdFPqQmTNfLtRF6TVsaJfc7vmA
"""

"""
DAG для расчета среднего количества лайков и комментариев на пост для каждой темы.
Вариант задания №16

Автор: Нургалеева Гузель
Дата: 2025
"""

from datetime import datetime, timedelta
import pandas as pd
import json
import sqlite3
import os
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from airflow.utils.dates import days_ago

# Конфигурация по умолчанию для DAG
default_args = {
    'owner': 'student',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email': ['test@example.com']
}

# Создание DAG
dag = DAG(
    'blog_posts_likes_comments_analysis',
    default_args=default_args,
    description='Расчет среднего количества лайков и комментариев на пост для каждой темы',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['etl', 'blog_posts', 'likes', 'comments', 'variant_16']
)

# Пути к файлам данных
DATA_DIR = '/opt/airflow/dags/data'
DB_PATH = '/opt/airflow/blog_posts_likes_comments_analysis.db'

def extract_blog_posts_data(**context):
    """
    Extract: Чтение данных о приложениях из CSV файла
    """
    print("Начинаем извлечение данных о приложениях из CSV...")

    csv_path = os.path.join(DATA_DIR, 'blog_posts.csv')

    try:
        # Чтение CSV файла
        blog_posts_df = pd.read_csv(csv_path)
        print(f"Загружено {len(blog_posts_df)} записей о постах")
        print("Первые 5 записей:")
        print(blog_posts_df.head())

        # Сохранение данных для следующих задач
        blog_posts_data = blog_posts_df.to_dict('records')
        context['task_instance'].xcom_push(key='blog_posts_data', value=blog_posts_data)

        print("Данные о постах успешно извлечены и сохранены в XCom")
        return f"Извлечено {len(blog_posts_df)} записей о постах"

    except Exception as e:
        print(f"Ошибка при извлечении данных о постах: {str(e)}")
        raise

def extract_likes_data(**context):
    """
    Extract: Чтение данных о лайках из Excel файла
    """
    print("Начинаем извлечение данных о лайках из Excel...")

    excel_path = os.path.join(DATA_DIR, 'likes.xlsx')

    try:
        # Чтение Excel файла
        likes_df = pd.read_excel(excel_path)
        print(f"Загружено {len(likes_df)} записей о лайках")
        print("Первые 5 записей:")
        print(likes_df.head())

        # Сохранение данных для следующих задач
        likes_data = likes_df.to_dict('records')
        context['task_instance'].xcom_push(key='likes_data', value=likes_data)

        print("Данные о лайках успешно извлечены и сохранены в XCom")
        return f"Извлечено {len(likes_df)} записей о лайках"

    except Exception as e:
        print(f"Ошибка при извлечении данных об установках: {str(e)}")
        raise

def extract_comments_w_likes_data(**context):
    """
    Extract: Чтение данных о комментариях от лайкнувших пользователей из JSON файла
    """
    print("Начинаем извлечение данных о комментариях от лайкнувших пользователей из JSON...")

    json_path = os.path.join(DATA_DIR, 'comments_w_likes.json')

    try:
        # Чтение JSON файла
        with open(json_path, 'r', encoding='utf-8') as f:
            comments_w_likes_data = json.load(f)

        comments_w_likes_df = pd.DataFrame(comments_w_likes_data)
        print(f"Загружено {len(comments_w_likes_df)} записей о о комментариях от лайкнувших пользователей")
        print("Первые 5 записей:")
        print(comments_w_likes_df.head())

        # Сохранение данных для следующих задач
        context['task_instance'].xcom_push(key='comments_w_likes_data', value=comments_w_likes_data)

        print("Данные о комментариях от лайкнувших пользователей успешно извлечены и сохранены в XCom")
        return f"Извлечено {len(comments_w_likes_df)} записей о комментариях от лайкнувших пользователей"

    except Exception as e:
        print(f"Ошибка при извлечении данных о  комментариях от лайкнувших пользователей: {str(e)}")
        raise

def extract_comments_wo_likes_data(**context):
    """
    Extract: Чтение данных о комментариях от не лайкнувших пользователей из JSON файла
    """
    print("Начинаем извлечение данных о комментариях от не лайкнувших пользователей из JSON...")

    json_path = os.path.join(DATA_DIR, 'comments_wo_likes.json')

    try:
        # Чтение JSON файла
        with open(json_path, 'r', encoding='utf-8') as f:
            comments_wo_likes_data = json.load(f)

        comments_wo_likes_df = pd.DataFrame(comments_wo_likes_data)
        print(f"Загружено {len(comments_wo_likes_df)} записей о комментариях от не лайкнувших пользователей")
        print("Первые 5 записей:")
        print(comments_wo_likes_df.head())

        # Сохранение данных для следующих задач
        context['task_instance'].xcom_push(key='comments_wo_likes_data', value=comments_wo_likes_data)

        print("Данные о комментариях от не лайкнувших пользователей успешно извлечены и сохранены в XCom")
        return f"Извлечено {len(comments_wo_likes_df)} записей о комментариях от не лайкнувших пользователей"

    except Exception as e:
        print(f"Ошибка при извлечении данных о комментариях от не лайкнувших пользователей: {str(e)}")
        raise

def transform_data(**context):
    """
    Transform: Консолидация данных и расчет среднего количества лайков и комментариев на тему поста
    """
    print("Начинаем трансформацию данных...")

    try:
        # Получение данных из предыдущих задач
        blog_posts_data = context['task_instance'].xcom_pull(key='blog_posts_data', task_ids='extract_blog_posts')
        likes_data = context['task_instance'].xcom_pull(key='likes_data', task_ids='extract_likes')
        comments_w_likes_data = context['task_instance'].xcom_pull(key='comments_w_likes_data', task_ids='extract_comments_w_likes')
        comments_wo_likes_data = context['task_instance'].xcom_pull(key='comments_wo_likes_data', task_ids='extract_comments_wo_likes')

        # Преобразование в DataFrame
        blog_posts_df = pd.DataFrame(blog_posts_data)
        likes_df = pd.DataFrame(likes_data)
        comments_w_likes_df = pd.DataFrame(comments_w_likes_data)
        comments_wo_likes_df = pd.DataFrame(comments_wo_likes_data)

        print("Данные успешно получены из XCom")
        print(f"Посты: {len(blog_posts_df)} записей")
        print(f"Лайки: {len(likes_df)} записей")
        print(f"Комментарии: {len(comments_w_likes_df)} записей")
        print(f"Комментарии без лайков: {len(comments_wo_likes_df)} записей")

        # Объединение данных
        # сначала объединю два вида комментариев по вертикали
        comments_df = pd.concat([comments_w_likes_df, comments_wo_likes_df], axis=0).reset_index() #.drop({'index'}, axis = 1)

        # объедиение всех датасетов в один. горизонтальное объединение

        # Объединение с лайками
        merged_df = pd.merge(blog_posts_df, likes_df, on = 'post_id', how = "left")

        # объединение с комментариями
        merged_df_2 = pd.merge(merged_df, comments_df, on='post_id', how = "left", suffixes = ['_likes', '_all_comments'])

        # Ну и раз сделан набор данных для комментариев от нелайкнувших пользователей, то можно их присоединить отдельно и определить их долю
        final_df = pd.merge(merged_df_2, comments_wo_likes_df.rename({'user_id':"user_id_comments_wo_likes"}, axis = 1), on = 'post_id', how = "left")
        print(f"После объединения: {len(final_df)} записей")

        # Расчет по постам
        print("Начинаем расчет количества уникальных user_id для каждого поста (как по лайкам, так и по комментариям)...")

        blog_posts_analysis = final_df.groupby(['topic', 'post_id']).agg({
            'user_id_likes': 'nunique',
            'user_id_all_comments': 'nunique',
            'user_id_comments_wo_likes': 'nunique'
        }).reset_index()


        # Расчет по топикам
        blog_posts_analysis_by_topics = blog_posts_analysis.groupby(['topic']).agg({
            'post_id': 'nunique',
            'user_id_likes': 'mean',
            'user_id_all_comments': 'mean',
            'user_id_comments_wo_likes': 'mean'
        }).reset_index()

        # Переименование колонок для ясности
        blog_posts_analysis_by_topics.columns = ['topic', 'total_posts', 'avg_likes', 'avg_comments', 'avg_comments_wo_likes']
        blog_posts_analysis_by_topics['comments_wo_likes_share'] = blog_posts_analysis_by_topics['avg_comments_wo_likes']/blog_posts_analysis_by_topics['avg_comments']

        print("Результаты расчета среднего количества лайков и комментариев на пост для каждой темы:")
        print(blog_posts_analysis_by_topics)

        # Сохранение результатов для загрузки в БД
        result_data = blog_posts_analysis_by_topics.to_dict('records')
        context['task_instance'].xcom_push(key='blog_posts_analysis_by_topics', value = result_data)

        print("Трансформация данных завершена успешно")
        return f"Проанализировано {len(blog_posts_analysis_by_topics)} топиков"

    except Exception as e:
        print(f"Ошибка при трансформации данных: {str(e)}")
        raise

def load_to_database(**context):
    """
    Load: Загрузка результатов анализа в SQLite базу данных
    """
    print("Начинаем загрузку данных в базу данных...")

    try:
        # Получение результатов анализа
        blog_posts_analysis_by_topics_data = context['task_instance'].xcom_pull(
                                  key='blog_posts_analysis_by_topics',
                                  task_ids='transform_data'
                                  )

        if not blog_posts_analysis_by_topics_data:
            raise ValueError("Нет данных для загрузки в базу данных")

        # Создание DataFrame из результатов
        blog_posts_analysis_by_topics_df = pd.DataFrame(blog_posts_analysis_by_topics_data)

        # Подключение к SQLite базе данных
        conn = sqlite3.connect(DB_PATH)
        try:
            # Создание таблицы если она не существует
            create_table_query = """
            CREATE TABLE IF NOT EXISTS blog_posts_analysis_by_topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                topic TEXT NOT NULL,
                total_posts INTEGER NOT NULL,
                avg_likes  NUMERIC(10,2) NOT NULL,
                avg_comments NUMERIC(10,2)NOT NULL,
                avg_comments_wo_likes NUMERIC(10,2) NOT NULL,
                comments_wo_likes_share NUMERIC(10,2) NOT NULL
            )
            """
            conn.execute(create_table_query)

            # Очистка таблицы перед загрузкой новых данных
            conn.execute("DELETE FROM blog_posts_analysis_by_topics")

            # Загрузка данных в таблицу
            blog_posts_analysis_by_topics_df.to_sql('blog_posts_analysis_by_topics', conn, if_exists='append', index=False)

            # Подтверждение транзакции
            conn.commit()

            print(f"Успешно загружено {len(blog_posts_analysis_by_topics_df)} записей в базу данных")

            # Проверка загруженных данных
            verification_query = "SELECT * FROM blog_posts_analysis_by_topics ORDER BY avg_likes DESC"
            result = pd.read_sql_query(verification_query, conn)
            print("Проверка загруженных данных:")
            print(result)

        finally:
            conn.close()

        print("Загрузка в базу данных завершена успешно")
        return f"Загружено {len(blog_posts_analysis_by_topics_df)} записей в SQLite базу данных"

    except Exception as e:
        print(f"Ошибка при загрузке в базу данных: {str(e)}")
        raise

def generate_report(**context):
    """
    Генерация отчета с результатами анализа и сохранение в файл
    """
    print("Генерируем отчет с результатами анализа...")

    try:
        # Получение данных из базы данных
        conn = sqlite3.connect(DB_PATH)

        try:
            query = """
            SELECT
                topic,
                total_posts,
                avg_likes,
                avg_comments,
                avg_comments_wo_likes,
                comments_wo_likes_share
            FROM blog_posts_analysis_by_topics
            ORDER BY avg_likes DESC
            """

            result_df = pd.read_sql_query(query, conn)

            # Формирование отчета
            report = f"""ОТЧЕТ ПО РАСЧЕТУ СРЕДНЕГО КОЛИЧЕСТВА ЛАЙКОВ И КОММЕНТАРИЕВ НА ТОПИК
================================================================

Дата анализа: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}


РЕЗУЛЬТАТЫ ПО ТОПИКАМ:
"""

            for _, row in result_df.iterrows():
                report += f"""
Категория: {row['topic']}
- Общее количество постов: {row['total_posts']:,}
- Среднее количество лайков на пост: {row['avg_likes']:,}
- Среднее количество комментариев на пост: {row['avg_comments']:,}
- Среднее количество комментариев на пост, среди не поставивших лайк пользователей: {row['avg_comments_wo_likes']:,}
- Доля пользователей оставивших комментарии, но не поставивиших лайк, в общем количестве комментариев: {row['comments_wo_likes_share']*100:.2f}%
"""

            # Добавление общей статистики
            total_topics = result_df['topic'].count()
            avg_posts = result_df['total_posts'].mean()

            # создание вспомогательного DataFrame для допрасчетов, чтобы не изменять финальный датасет с результатами
            assist_result_df = result_df.copy()
            assist_result_df['total_avg_likes']=assist_result_df['avg_likes']*assist_result_df['total_posts']
            assist_result_df['total_avg_comments']=assist_result_df['avg_comments']*assist_result_df['total_posts']

            avg_likes = assist_result_df['total_avg_likes'].sum()/assist_result_df['total_posts'].sum()
            avg_comments = assist_result_df['total_avg_comments'].sum()/assist_result_df['total_posts'].sum()

            report += f"""
ОБЩАЯ СТАТИСТИКА:
- Общее количество топиков (тем): {total_topics:,}
- Среднее количество постов в теме: {avg_posts:,}
- Среднее количество лайков по всем темам: {avg_likes:,}
- Среднее количество комментариев по всем темам: {avg_comments:,}


РЕКОМЕНДАЦИИ:
"""

            # Добавление рекомендаций на основе анализа
            best_topic = result_df.iloc[0]
            worst_topic = result_df.iloc[-1]

            report += f"""- Лучшая тема по количеству лайков "{best_topic['topic']}" ({best_topic['avg_likes']})
- Требует внимания тема "{worst_topic['topic']}" ({worst_topic['avg_likes']})
"""

            print("Отчет сгенерирован:")
            print(report)

            # Сохранение отчета в файл
            report_file_path = '/opt/airflow/blog_posts_analysis_report.txt'
            with open(report_file_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"Отчет сохранен в файл: {report_file_path}")

            # Сохранение CSV файла с данными
            csv_file_path = '/opt/airflow/blog_posts_analysis_data.csv'
            result_df.to_csv(csv_file_path, index=False, encoding='utf-8')
            print(f"Данные сохранены в CSV: {csv_file_path}")

            # Сохранение данных для email
            context['task_instance'].xcom_push(key='report', value=report)
            context['task_instance'].xcom_push(key='report_file_path', value=report_file_path)
            context['task_instance'].xcom_push(key='csv_file_path', value=csv_file_path)
            context['task_instance'].xcom_push(key='result_data', value=result_df.to_dict('records'))

        finally:
            conn.close()

        return "Отчет успешно сгенерирован и сохранен в файлы"

    except Exception as e:
        print(f"Ошибка при генерации отчета: {str(e)}")
        raise

# Определение задач DAG

# Extract задачи
extract_blog_posts_task = PythonOperator(
    task_id='extract_blog_posts',
    python_callable = extract_blog_posts_data,
    dag=dag,
    doc_md="""
    ### Извлечение данных о поставх, авторах, темах
    Читает CSV файл.
    """
)

extract_likes_task = PythonOperator(
    task_id='extract_likes',
    python_callable = extract_likes_data,
    dag=dag,
    doc_md="""
    ### Извлечение данных о лайках (post_id, user_id)
    Читает Excel файл.
    """
)

extract_comments_w_likes_task = PythonOperator(
    task_id='extract_comments_w_likes',
    python_callable = extract_comments_w_likes_data,
    dag=dag,
    doc_md="""
    ### Извлечение данных о комментариях тех пользователей, которые также поставили лайк
    Читает JSON файл.
    """
)

extract_comments_wo_likes_task = PythonOperator(
    task_id='extract_comments_wo_likes',
    python_callable = extract_comments_wo_likes_data,
    dag=dag,
    doc_md="""
    ### Извлечение данных о комментариях тех пользователей, которые не поставили лайк
    Читает JSON файл.
    """
)

# Transform задача
transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
    doc_md="""
    ### Трансформация данных
    Объединяет данные из всех источников и рассчитывает среднее количество лайков и комментариев на тему.
    """
)

# Load задача
load_task = PythonOperator(
    task_id='load_to_database',
    python_callable=load_to_database,
    dag=dag,
    doc_md="""
    ### Загрузка в базу данных
    Сохраняет результаты анализа в SQLite базу данных.
    """
)

# Генерация отчета
report_task = PythonOperator(
    task_id='generate_report',
    python_callable=generate_report,
    dag=dag,
    doc_md="""
    ### Генерация отчета
    Создает детальный отчет с результатами анализа.
    """
)

def send_email_with_attachments(**context):
    """
    Отправка email с прикрепленными файлами результатов
    """
    from airflow.utils.email import send_email
    import os

    try:
        # Получение данных из предыдущих задач
        report = context['task_instance'].xcom_pull(key='report', task_ids='generate_report')
        result_data = context['task_instance'].xcom_pull(key='result_data', task_ids='generate_report')

        # Формирование HTML содержимого с результатами
        html_content = f"""
        <h2>🎉 Расчет среднего количества лайков и комментариев на пост для каждой темы!</h2>

        <h3>📊 Информация о выполнении:</h3>
        <ul>
            <li><strong>DAG:</strong> blog_posts_likes_comments_analysis</li>
            <li><strong>Дата выполнения:</strong> {context['ds']}</li>
            <li><strong>Статус:</strong> ✅ Все задачи выполнены без ошибок</li>
            <li><strong>Результаты:</strong> Сохранены в базе данных SQLite</li>
        </ul>

        <h3>📈 Краткие результаты анализа:</h3>
        <table border="1" style="border-collapse: collapse; width: 100%;">
            <tr style="background-color: #f2f2f2;">
                <th>Тема</th>
                <th>Количество постов</th>
                <th>Среднее количество лайков</th>
                <th>Среднее количество комментариев</th>
            </tr>
        """

        if result_data:
            for row in result_data:
                html_content += f"""
            <tr>
                <td>{row['topic']}</td>
                <td>{row['total_posts']:,}</td>
                <td>{row['avg_likes']:,}</td>
                <td>{row['avg_comments']:}</td>
            </tr>
                """

        html_content += """
        </table>

        <h3>📎 Прикрепленные файлы:</h3>
        <ul>
            <li><strong>blog_posts_analysis_report.txt</strong> - Подробный текстовый отчет</li>
            <li><strong>blog_posts_analysis_data.csv</strong> - Данные в формате CSV</li>
        </ul>

        <p><em>Детальный отчет также доступен в логах задачи generate_report в Airflow UI.</em></p>

        <hr>
        <p style="color: #666; font-size: 12px;">
            Это автоматическое уведомление от системы Apache Airflow<br>
            Время отправки: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        </p>
        """

        # Подготовка файлов для отправки
        files = []
        report_file = '/opt/airflow/blog_posts_analysis_report.txt'
        csv_file = '/opt/airflow/blog_posts_analysis_data.csv'

        if os.path.exists(report_file):
            files.append(report_file)
            print(f"Добавлен файл для отправки: {report_file}")

        if os.path.exists(csv_file):
            files.append(csv_file)
            print(f"Добавлен файл для отправки: {csv_file}")

        # Отправка email
        send_email(
            to=['test@example.com'],
            subject='📊 Расчет среднего количества лайков и комментариев на пост для каждой темы - Результаты',
            html_content=html_content,
            files=files
        )

        print("Email с результатами и прикрепленными файлами отправлен успешно!")
        return "Email отправлен с прикрепленными файлами"

    except Exception as e:
        print(f"Ошибка при отправке email: {str(e)}")
        # Отправляем базовое уведомление без файлов
        send_email(
            to=['test@example.com'],
            subject='⚠️ Расчет среднего количества лайков и комментариев на пост для каждой темы - Завершен (без файлов)',
            html_content=f"""
            <h3>Расчет среднего количества лайков и комментариев на пост для каждой темы завершен успешно!</h3>
            <p>DAG: blog_posts_likes_comments_analysis</p>
            <p>Дата выполнения: {context['ds']}</p>
            <p>Все задачи выполнены без ошибок.</p>
            <p><strong>Примечание:</strong> Файлы результатов не удалось прикрепить из-за ошибки: {str(e)}</p>
            <p>Результаты доступны в логах задачи generate_report.</p>
            """
        )
        raise

# Email уведомление с файлами
email_task = PythonOperator(
    task_id='send_email_notification',
    python_callable=send_email_with_attachments,
    dag=dag,
    doc_md="""
    ### Отправка email-уведомления
    Отправляет email с результатами анализа и прикрепленными файлами.
    """
)

# Определение зависимостей между задачами
# Extract задачи выполняются параллельно
[extract_blog_posts_task, extract_likes_task, extract_comments_w_likes_task, extract_comments_wo_likes_task] >> transform_task

# Transform -> Load -> Report -> Email (последовательно)
transform_task >> load_task >> report_task >> email_task
