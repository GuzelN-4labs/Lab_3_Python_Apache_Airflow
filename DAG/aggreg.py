# -*- coding: utf-8 -*-
"""aggreg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gbnZGRlEgpBvEevtqBlZEXT0PKtBbiyD
"""

from airflow import DAG
from airflow.operators.python_operator import PythonOperator # type: ignore
from datetime import datetime
import pandas as pd # type: ignore
import random
import numpy as np
import os
import json

# Настройка генератора случайных чисел для воспроизводимости
np.random.seed(42)
random.seed(42)

""" создание списка тем для постов"""
post_topics = ['Economy', 'Ecology', 'Fashion', 'Health', 'Education', 'Technology', 'Environment', 'Travelling', 'Sports', 'Ethics', 'Arts']


def generate_author_id():
    """генерация id авторов"""
    authors_data = []
    for i in range(1,101):
        authors = {
            "author_id": f"author_{i:03d}"
        }

        authors_data.append(authors)
    return(authors_data)

def generate_blog_posts(authors_data):
    """ генерация id постов с одновременным выбором автора, чтобы предотвратить дублирование post_id у разных авторов"""
    posts_data = []
    posts_counter = 1
    for i in range(1500): # генерация 1500 номеров постов
        author = random.choice(authors_data)
        post_theme = random.choice(post_topics)
        posts_details = {
            "post_id": f"post_{posts_counter:05d}",
            "topic": post_theme,
            "author_id": author['author_id']
        }
        posts_data.append(posts_details)
        posts_counter +=1
    df_blog_posts = pd.DataFrame(posts_data)
    df_blog_posts.to_csv('data/blog_posts.csv', index = False, encoding = 'utf-8')
    print("Файл с блог-постами создан (csv)")
    return posts_data


def generate_likes(posts_data):
    """генерируем лайки, чтобы разные посты пересекались по user_id, и в пределах поста юзеры не дублровались"""
    likes_data = []
    for posts_details in posts_data:
        post_id = posts_details['post_id']
        likes_qty = random.randint(1, 150)
        # Generate a set of unique user IDs for this post
        users_for_post = set()
        while len(users_for_post) < likes_qty:
            users_for_post.add(f"user_{random.randint(1, 5000):05d}")

        for user_id in users_for_post:
            likes_details = {
                "post_id": post_id,
                "user_id": user_id
            }
            likes_data.append(likes_details)

    df_likes = pd.DataFrame(likes_data)
    df_likes.to_excel('data/likes.xlsx', index=False)
    print("файл с лайками создан (excel)")
    return likes_data


def generate_comments_w_likes(posts_data,likes_data):
    """ генерируем комментарии среди тех пользователей, которые лайкали """
    comments_data_w_likes = []
    df_assist = pd.DataFrame(likes_data)
    likes_qty = 0
    for posts_details in posts_data:
        post_id = posts_details['post_id']
        likes_qty = df_assist[df_assist['post_id'] ==post_id]['user_id'].nunique()
        likes_list = df_assist[df_assist['post_id'] ==post_id]['user_id'].unique()
        comments_for_post = set()
        while len(comments_for_post) < likes_qty*0.7:
            comments_for_post.add(random.choice(likes_list))

        for user_id in comments_for_post:
            comments_details = {
                "post_id": post_id,
                "user_id": user_id
            }
            comments_data_w_likes.append(comments_details)

    # Сохраняем в JSON
    with open('data/comments_w_likes.json', 'w', encoding='utf-8') as f:
        json.dump(comments_data_w_likes, f, ensure_ascii=False, indent=2)

    print("файл с комментариями от лайкнувших пользователей создан (json)")
    return comments_data_w_likes

def generate_comments_wo_likes(posts_data, likes_data):
    """ генерируем комментарии среди тех пользователей, которые не лайкали """
    comments_data_wo_likes = []
    df_assist = pd.DataFrame(likes_data)
    comments_for_post_qty = 0
    for posts_details in posts_data:
        post_id = posts_details['post_id']
        likes_list = df_assist[df_assist['post_id'] ==post_id]['user_id'].unique()
        comments_for_post_qty = random.randint(1,5)
        comments_for_post = set()
        while len(comments_for_post) <= comments_for_post_qty:
            new_user = f"user_{random.randint(1, 5000):05d}"
            if new_user not in likes_list:
                comments_for_post.add(new_user)
            else:
                continue

        for user_id in comments_for_post:
            comments_details = {
                "post_id": post_id,
                "user_id": user_id
            }
            comments_data_wo_likes.append(comments_details)


    # Сохраняем в JSON
    with open('data/comments_wo_likes.json', 'w', encoding='utf-8') as f:
        json.dump(comments_data_wo_likes, f, ensure_ascii=False, indent=2)

    print("файл с комментариями от нелайкнувших пользователей создан (json)")
    return comments_data_wo_likes

def main():
    """ основная функция генерации данных, которая будет включать ранее соданные функции и запускать их """
    """ создавать папку data, сохранять в нее сгенерированные файлы """
    if not os.path.exists('data'):
        os.makedirs('data')
        print('папка data создана')

    authors_data = generate_author_id()
    blog_posts = generate_blog_posts(authors_data)
    likes = generate_likes(blog_posts)
    comments_w_likes = generate_comments_w_likes(blog_posts, likes)
    comments_wo_likes = generate_comments_wo_likes(blog_posts, likes)


if __name__ == "__main__":
    main()

'''
# Функция для агрегации данных из всех трех файлов
def aggregate_data():
    # Загрузка данных из файлов
    df_blog_posts = pd.read_csv('data/blog_posts.csv')
    df_likes = pd.read_excel('data/likes.xlsx')

    with open('data/comments_wo_likes.json', 'r', encoding='utf-8') as f:
        comments_wo_likes = json.load(f)
    df_comments_1 = pd.DataFrame(comments_wo_likes)

    with open('data/comments_w_likes.json', 'r', encoding='utf-8') as f:
        comments_w_likes = json.load(f)
    df_comments_2 = pd.DataFrame(comments_w_likes)

    # объединение двух частей комментариев (два датафрейма в один) вертикальное объединение
    df_comments = pd.concat([df_comments_1, df_comments_2], axis=0).reset_index().drop({'index'}, axis = 1)

    # объедиение всех датасетов в один. горизонтальное объединение
    df_merged = pd.merge(df_blog_posts, df_likes, on = 'post_id', how = "left")
    df_merged = pd.merge(df_merged, df_comments, on='post_id', how = "left", suffixes = ['_likes', '_comments'])

    # проведение агрегации
    aggregated_df = df_merged.groupby(['topic','post_id']).agg({'user_id_likes': 'nunique', 'user_id_comments': 'nunique'}).reset_index()
    aggregared_df = aggregated_df.groupby(['topic']).agg({'user_id_likes': 'mean', 'user_id_comments': 'mean'})

    aggregated_df.to_csv(f'aggregated_data.csv', index=False, encoding='utf-8')

# Определение DAG
dag = DAG('generate_data_dag', description='Generate and aggregate sample data',
          schedule_interval='@once', start_date=datetime(2023, 10, 1), catchup=False)

# Определение задач
task_generate_all_data = PythonOperator(task_id = 'main', python_callable = main, dag = dag)
task_aggregate_data = PythonOperator(task_id = 'aggregate_data', python_callable = aggregate_data, dag = dag)

# Установка зависимостей между задачами
task_generate_all_data >> task_aggregate_data

'''
